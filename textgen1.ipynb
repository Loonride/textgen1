{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pynvml\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Device 0 : b'GeForce GTX 1650'\n",
      "Mem: 8.69/15.49 GB, GPUs: 1.28/3.82 GB, \n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cpu = 'cpu'\n",
    "print(device)\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "deviceCount = pynvml.nvmlDeviceGetCount()\n",
    "for i in range(deviceCount):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    print(\"Device\", i, \":\", pynvml.nvmlDeviceGetName(handle))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def get_mem_msg():\n",
    "    mem_info = psutil.virtual_memory()\n",
    "    used = round(mem_info.used / 1024 ** 3, 2)\n",
    "    total = round(mem_info.total / 1024 ** 3, 2)\n",
    "    msg = f'Mem: {used}/{total} GB, GPUs: '\n",
    "\n",
    "    for i in range(deviceCount):\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        used = round(mem_info.used / 1024 ** 3, 2)\n",
    "        total = round(mem_info.total / 1024 ** 3, 2)\n",
    "        msg += f'{used}/{total} GB, '\n",
    "\n",
    "    return msg\n",
    "\n",
    "print(get_mem_msg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = [\n",
    "    'gatsby.txt',\n",
    "    'alice.txt',\n",
    "    'frankenstein.txt',\n",
    "    'grimm.txt',\n",
    "    'sherlock.txt',\n",
    "]\n",
    "\n",
    "txt = \"\"\n",
    "for b in books:   \n",
    "    with open(b, 'r') as f:\n",
    "        txt += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', '½', 'à', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ô', 'ù', 'œ', '\\u200a', '—', '‘', '“', '”', '…', '\\ufeff']\n",
      "[13731, 10615, 6339, 4431, 9416, 13731, 6188, 5907, 18, 1940, 5069, 19, 11923, 5405, 13801, 4431, 7561, 5571, 13731, 14652, 9416, 684, 687, 7101, 13731, 14504, 13014, 613, 8925, 9555, 9806, 9416, 13731, 15404, 942, 9215, 3102, 613, 15329, 528, 9215, 11404, 15135, 19, 15512, 8554, 3055, 7578, 18, 6000, 7578, 1056, 9507, 10974, 14652, 7578, 14404, 13731, 13692, 9416, 13731, 10615, 6339, 8096, 7137, 15329, 13801, 4431, 9507, 9468, 942, 15471, 19, 6339, 19, 9525, 19, 6957, 15512, 784, 9269, 8225, 7101, 13731, 14504, 13014, 18, 15512, 15253, 6506, 13940, 2280, 13731, 7970, 9416, 13731, 3138, 15147, 15512, 784]\n"
     ]
    }
   ],
   "source": [
    "# txt = txt.replace(\"’\", \"'\")\n",
    "# txt = txt.replace('—', '-')\n",
    "txt = txt.replace('’', \"'\")\n",
    "# txt = txt.replace('“', '\"')\n",
    "# txt = txt.replace('”', '\"')\n",
    "\n",
    "# tokens = nltk.word_tokenize(txt)\n",
    "# print(tokens[:10000])\n",
    "\n",
    "chars = sorted(list(set(txt)))\n",
    "print(chars)\n",
    "\n",
    "txt = txt.lower()\n",
    "words = re.findall(r\"mr\\.|mrs\\.|[a-z0-9']+|[.,!?;:]\", txt)\n",
    "\n",
    "words_set = sorted(set(words))\n",
    "\n",
    "words_nums = []\n",
    "for w in words:\n",
    "    words_nums.append(words_set.index(w))\n",
    "print(words_nums[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  429250\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(words_nums) - seq_length, 1):\n",
    "    seq_in = words_nums[i:i + seq_length]\n",
    "    seq_out = words_nums[i + seq_length]\n",
    "    dataX.append(seq_in)\n",
    "    dataY.append(seq_out)\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.88381823]\n",
      "  [0.6832518 ]\n",
      "  [0.40802008]\n",
      "  ...\n",
      "  [0.3802137 ]\n",
      "  [0.0011586 ]\n",
      "  [0.12487127]]\n",
      "\n",
      " [[0.6832518 ]\n",
      "  [0.40802008]\n",
      "  [0.28520855]\n",
      "  ...\n",
      "  [0.0011586 ]\n",
      "  [0.12487127]\n",
      "  [0.32627446]]\n",
      "\n",
      " [[0.40802008]\n",
      "  [0.28520855]\n",
      "  [0.60607621]\n",
      "  ...\n",
      "  [0.12487127]\n",
      "  [0.32627446]\n",
      "  [0.00122297]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.44007467]\n",
      "  [0.89727085]\n",
      "  [0.85298661]\n",
      "  ...\n",
      "  [0.89727085]\n",
      "  [0.42127961]\n",
      "  [0.01107106]]\n",
      "\n",
      " [[0.89727085]\n",
      "  [0.85298661]\n",
      "  [0.89727085]\n",
      "  ...\n",
      "  [0.42127961]\n",
      "  [0.01107106]\n",
      "  [0.59037075]]\n",
      "\n",
      " [[0.85298661]\n",
      "  [0.89727085]\n",
      "  [0.61553811]\n",
      "  ...\n",
      "  [0.01107106]\n",
      "  [0.59037075]\n",
      "  [0.28527291]]]\n",
      "[ 5069    19 11923 ...  9172  4432    19]\n"
     ]
    }
   ],
   "source": [
    "n_words_set = len(words_set)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_words_set)\n",
    "# one hot encode the output variable\n",
    "y = np.array(dataY)\n",
    "# y = np_utils.to_categorical(dataY)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenDataset(Dataset):\n",
    "    def __init__(self, input_seqs, target_labels, n_patterns):\n",
    "        self.input_seqs = input_seqs\n",
    "        self.target_labels = target_labels\n",
    "        self.n_patterns = n_patterns\n",
    "    \n",
    "    def __len__(self):\n",
    "        return n_patterns\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_seq = torch.tensor(self.input_seqs[index], dtype=torch.float)\n",
    "        target_label = torch.tensor(self.target_labels[index])\n",
    "        return (input_seq, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15536\n"
     ]
    }
   ],
   "source": [
    "text_gen_train = TextGenDataset(X, y, n_patterns)\n",
    "text_gen_train_loader = torch.utils.data.DataLoader(dataset=text_gen_train,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True)\n",
    "print(n_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\" Custom CNN-LSTM model for sequence prediction problem \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Define and instantiate your layers\"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(1, 512, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(512, 2048, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 8192)\n",
    "        self.fc2 = nn.Linear(8192, n_words_set)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out_last = out[:,-1,:]\n",
    "        res = F.relu(self.fc1(out_last))\n",
    "        res = self.fc2(res)\n",
    "        res = self.log_softmax(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_words_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b997eb798a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-ade9b792a820>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_words_set' is not defined"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"training_data\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "log_interval = 10\n",
    "\n",
    "model = LSTM()\n",
    "model = model.to(device)\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00025)\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    for epoch in range(100):\n",
    "        save_path = self.save_dir / f\"net_{epoch}.p\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        total_loss = 0\n",
    "        # sets training mode if we are doing dropout when training\n",
    "        model.train()\n",
    "        for batch_idx, (input_seqs, target_labels) in enumerate(text_gen_train_loader):\n",
    "            input_seqs = input_seqs.to(device)\n",
    "            target_labels = target_labels.to(device)\n",
    "\n",
    "            res = model(input_seqs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(res, target_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % log_interval == log_interval - 1:\n",
    "                torch.cuda.empty_cache()\n",
    "                avg_loss = total_loss / log_interval\n",
    "                mem_msg = get_mem_msg()\n",
    "                print(f'epoch: {epoch}, loss: {avg_loss}, {mem_msg}')\n",
    "                total_loss = 0\n",
    "\n",
    "train(model)\n",
    "print('Training Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
